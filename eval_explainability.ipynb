{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import open3d as o3d\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from final_models_explainability.get_predictions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/scratch-disk/full-datasets/hcampus-1.5T-cohort-holdout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability evaluation by comparing left and right hippocampal attributions\n",
    "### Evaluating left and right hippocampus model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = find_subjects_parallel(data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subject_list:\n",
    "\n",
    "    subject.data = {}\n",
    "\n",
    "    # Group\n",
    "    mapping = {\n",
    "        'CN': 0,\n",
    "        'MCI': 1,\n",
    "    }\n",
    "\n",
    "    # Get the value of the mapping, -1 if not found\n",
    "    subject.data['research_group'] = mapping.get(subject.subject_metadata['Research Group'].iloc[0], -1)\n",
    "\n",
    "    # Cloud\n",
    "    subject.data['hcampus_pointcloud_aligned'] = np.load(os.path.join(subject.path, 'Left-Hippocampus_Right-Hippocampus_aligned_cropped_mesh_downsampledcloud.npy'))\n",
    "\n",
    "    # Volumes\n",
    "    volume_col = subject.aseg_stats['Volume_mm3']\n",
    "    volume_col_normalised = volume_col / volume_col.sum() * 1000\n",
    "    struct_name_col = subject.aseg_stats['StructName']\n",
    "    \n",
    "    subject.data['volumes'] = np.array(volume_col_normalised)\n",
    "    \n",
    "    subject.data['struct_names'] = np.array(struct_name_col)\n",
    "\n",
    "    # Scores\n",
    "    mmse = subject.subject_metadata['MMSE Total Score'].iloc[0]\n",
    "    gdscale = subject.subject_metadata['GDSCALE Total Score'].iloc[0]\n",
    "    faq = subject.subject_metadata['FAQ Total Score'].iloc[0]\n",
    "    npiq = subject.subject_metadata['NPI-Q Total Score'].iloc[0]\n",
    "\n",
    "    subject.data['scores'] = [mmse, gdscale, faq, npiq]\n",
    "\n",
    "    subject.data['score_names'] = ['MMSE Total Score', 'GDSCALE Total Score', 'FAQ Total Score', 'NPI-Q Total Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "pred_probs = []\n",
    "pred_classes = []\n",
    "data = []\n",
    "attributions_list = []\n",
    "\n",
    "# Run multiple times to account for randomness\n",
    "for i in range(5):\n",
    "\n",
    "    for subject in tqdm(subject_list, total=len(subject_list)):\n",
    "\n",
    "        model = pointnet2_cls_msg.get_model(2, normal_channel=False)\n",
    "\n",
    "        model.load_state_dict(torch.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/lr_eval_pointnet.pth\", weights_only=True))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        model.to('cuda')\n",
    "\n",
    "        input = subject.data['hcampus_pointcloud_aligned']\n",
    "\n",
    "        input = torch.from_numpy(input).type(torch.float32).to('cuda')\n",
    "\n",
    "        attributions = explain_pointnet(model, input)\n",
    "\n",
    "        input = input.unsqueeze(0).transpose(2, 1)\n",
    "\n",
    "        output = model(input)[0]\n",
    "\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        output = torch.nn.functional.softmax(output, dim=0)\n",
    "\n",
    "        pred_class = int(np.argmax(output.cpu().detach().numpy()))\n",
    "\n",
    "        output = output.cpu().detach().numpy()\n",
    "\n",
    "        output = output[1]\n",
    "\n",
    "        data.append(subject.data['hcampus_pointcloud_aligned'])\n",
    "\n",
    "        attributions_list.append(attributions)\n",
    "\n",
    "        true.append(subject.data['research_group'])\n",
    "        \n",
    "        pred_probs.append(output)\n",
    "\n",
    "        pred_classes.append(pred_class)\n",
    "\n",
    "np.savez('/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/lr_pointnet_eval.npz', true=true, pred_probs=pred_probs, pred_classes=pred_classes, data=data, attributions_list=attributions_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "report = classification_report(true, pred_classes, target_names=['CN', 'MCI'])\n",
    "\n",
    "print(report)\n",
    "\n",
    "roc_auc = roc_auc_score(true, pred_classes)\n",
    "\n",
    "print(f\"roc_auc: {roc_auc}\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(true, pred_probs)\n",
    "\n",
    "conf_matrix = confusion_matrix(true, pred_classes)\n",
    "\n",
    "conf_matrix_disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['CN','MCI'])\n",
    "\n",
    "conf_matrix_disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate explainability for correct positive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/lr_pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == 1 and results['pred_classes'][i] == 1:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_list': [results['attributions_list'][i] for i in correct_indices]\n",
    "                    \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "transformed_clouds_concat = []\n",
    "attributions_concat = []\n",
    "opacity_scalars_concat = []\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(10)\n",
    "\n",
    "o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "transformed_clouds_concat.extend(filtered_results['data'][0])\n",
    "\n",
    "norm_xyz_sum = normalise_attributions(filtered_results['attributions_list'][0], power = 0.2)\n",
    "\n",
    "attributions_concat.extend(norm_xyz_sum)\n",
    "\n",
    "opacity_scalars_concat.extend(np.clip(np.abs(norm_xyz_sum - 0.5), 0.05 , 0.8))\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_clouds_concat.extend(np.asarray(moving_pcd.points))\n",
    "\n",
    "    norm_xyz_sum = normalise_attributions(filtered_results['attributions_list'][i], power = 0.2)\n",
    "\n",
    "    attributions_concat.extend(norm_xyz_sum)\n",
    "\n",
    "    opacity_scalars_concat.extend(np.clip(np.abs(norm_xyz_sum - 0.5), 0.05 , 0.8))\n",
    "\n",
    "transformed_clouds_concat = np.array(transformed_clouds_concat)\n",
    "attributions_concat = np.array(attributions_concat)\n",
    "opacity_scalars_concat = np.array(opacity_scalars_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0634884947335203 0.03950413256988288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "cluster_labels = kmeans.fit_predict(transformed_clouds_concat)\n",
    "\n",
    "cluster_avg_attr = np.zeros_like(attributions_concat)\n",
    "\n",
    "cluster_0_mask = (cluster_labels == 0)\n",
    "cluster_1_mask = (cluster_labels == 1)\n",
    "\n",
    "points_0 = transformed_clouds_concat[cluster_0_mask]\n",
    "attributions_0 = attributions_concat[cluster_0_mask]\n",
    "opacities_0 = opacity_scalars_concat[cluster_0_mask]\n",
    "points_1 = transformed_clouds_concat[cluster_1_mask]\n",
    "attributions_1 = attributions_concat[cluster_1_mask]\n",
    "opacities_1 = opacity_scalars_concat[cluster_1_mask]\n",
    "\n",
    "mean_attr_0 = np.mean(np.abs(attributions_0 - 0.5))\n",
    "mean_attr_1 = np.mean(np.abs(attributions_1 - 0.5))\n",
    "\n",
    "print(mean_attr_0, mean_attr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea9b2664ece42f89da41db4c5046aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:34753/index.html?ui=P_0x7f139a05b920_10&reconnect=auto\" class=\"pyv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'blue'), (0.5, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "cloud_0 = pv.PolyData(points_0)\n",
    "cloud_1 = pv.PolyData(points_1)\n",
    "\n",
    "plotter.add_points(cloud_0, scalars=attributions_0, cmap=custom_cmap, clim= [0,1], opacity= opacities_0)\n",
    "\n",
    "plotter.add_points(cloud_1, scalars=attributions_1, cmap=custom_cmap, clim= [0,1], opacity= opacities_1)\n",
    "\n",
    "midpoint_0 = points_0.mean(axis=0)\n",
    "\n",
    "midpoint_1 = points_1.mean(axis=0)\n",
    "\n",
    "plotter.add_point_labels([midpoint_0.tolist()], [f\"Right: {mean_attr_0:.4f}\"], text_color='white')\n",
    "plotter.add_point_labels([midpoint_1.tolist()], [f\"Left: {mean_attr_1:.4f}\"], text_color='white')\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate explainability for all correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/lr_pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == results['pred_classes'][i]:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_list': [results['attributions_list'][i] for i in correct_indices]\n",
    "                    \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "transformed_clouds_concat = []\n",
    "attributions_concat = []\n",
    "opacity_scalars_concat = []\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(10)\n",
    "\n",
    "o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "transformed_clouds_concat.extend(filtered_results['data'][0])\n",
    "\n",
    "# Sum x, y and z values for an overall attribution for that point\n",
    "xyz_sum = np.sum(filtered_results['attributions_list'][0], axis=1)\n",
    "\n",
    "# Apply power for better vis\n",
    "xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), 0.3)\n",
    "\n",
    "xyz_sum = np.abs(xyz_sum)\n",
    "\n",
    "norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "opacity_scalars_concat.extend(np.clip(norm_xyz_sum, 0.01 , 0.8))\n",
    "\n",
    "attributions_concat.extend(norm_xyz_sum)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_clouds_concat.extend(np.asarray(moving_pcd.points))\n",
    "\n",
    "    # Sum x, y and z values for an overall attribution for that point\n",
    "    xyz_sum = np.sum(filtered_results['attributions_list'][i], axis=1)\n",
    "\n",
    "    # Apply power for better vis\n",
    "    xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), 0.3)\n",
    "\n",
    "    xyz_sum = np.abs(xyz_sum)\n",
    "\n",
    "    norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "    opacity_scalars_concat.extend(np.clip(norm_xyz_sum, 0.01 , 0.8))\n",
    "\n",
    "    attributions_concat.extend(norm_xyz_sum)\n",
    "\n",
    "transformed_clouds_concat = np.array(transformed_clouds_concat)\n",
    "attributions_concat = np.array(attributions_concat)\n",
    "opacity_scalars_concat = np.array(opacity_scalars_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08251925933492643 0.04679948698275074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "cluster_labels = kmeans.fit_predict(transformed_clouds_concat)\n",
    "\n",
    "cluster_avg_attr = np.zeros_like(attributions_concat)\n",
    "\n",
    "cluster_0_mask = (cluster_labels == 0)\n",
    "cluster_1_mask = (cluster_labels == 1)\n",
    "\n",
    "points_0 = transformed_clouds_concat[cluster_0_mask]\n",
    "attributions_0 = attributions_concat[cluster_0_mask]\n",
    "opacities_0 = opacity_scalars_concat[cluster_0_mask]\n",
    "points_1 = transformed_clouds_concat[cluster_1_mask]\n",
    "attributions_1 = attributions_concat[cluster_1_mask]\n",
    "opacities_1 = opacity_scalars_concat[cluster_1_mask]\n",
    "\n",
    "mean_attr_0 = np.mean(attributions_0)\n",
    "mean_attr_1 = np.mean(attributions_1)\n",
    "\n",
    "print(mean_attr_0, mean_attr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0b0a153fcb48708079f4e760e3f35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:34753/index.html?ui=P_0x7f1350efdc70_23&reconnect=auto\" class=\"pyv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "cloud_0 = pv.PolyData(points_0)\n",
    "cloud_1 = pv.PolyData(points_1)\n",
    "\n",
    "plotter.add_points(cloud_0, scalars=attributions_0, cmap=custom_cmap, clim= [0,1], opacity=opacities_0)\n",
    "\n",
    "plotter.add_points(cloud_1, scalars=attributions_1, cmap=custom_cmap, clim= [0,1], opacity=opacities_1)\n",
    "\n",
    "midpoint_0 = points_0.mean(axis=0)\n",
    "\n",
    "midpoint_1 = points_1.mean(axis=0)\n",
    "\n",
    "plotter.add_point_labels([midpoint_0.tolist()], [f\"Right: {mean_attr_0:.4f}\"], text_color='white')\n",
    "plotter.add_point_labels([midpoint_1.tolist()], [f\"Left: {mean_attr_1:.4f}\"], text_color='white')\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of pointnet explainability by global explainability\n",
    "### Global explainability for correct positive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointnet_eval = np.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/pointnet_eval.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == 1 and results['pred_classes'][i] == 1:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_list': [results['attributions_list'][i] for i in correct_indices]\n",
    "}\n",
    "\n",
    "print(len(filtered_results['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee820167bfb947f290d67fde43d1a96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:34753/index.html?ui=P_0x7f13bc386f30_19&reconnect=auto\" class=\"pyv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'blue'), (0.5, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "target_pv_cloud = pv.PolyData(filtered_results['data'][0])\n",
    "\n",
    "norm_xyz_sum = normalise_attributions(filtered_results['attributions_list'][0], power = 0.2)\n",
    "\n",
    "opacity_scalars = np.clip(np.abs(norm_xyz_sum - 0.5), 0.05 , 0.8)\n",
    "\n",
    "plotter.add_points(target_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim=[0,1], opacity=opacity_scalars)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_pv_cloud = pv.PolyData(np.asarray(moving_pcd.points))\n",
    "\n",
    "    norm_xyz_sum = normalise_attributions(filtered_results['attributions_list'][i], power = 0.2)\n",
    "\n",
    "    opacity_scalars = np.clip(np.abs(norm_xyz_sum - 0.5), 0.05 , 0.8)\n",
    "\n",
    "    plotter.add_points(transformed_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim= [0,1], opacity=opacity_scalars)\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global explainability for correct negative predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointnet_eval = np.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/pointnet_eval.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == 0 and results['pred_classes'][i] == 0:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_list': [results['attributions_list'][i] for i in correct_indices]\n",
    "}\n",
    "\n",
    "print(len(filtered_results['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38710ebe9064cabbc76fee2c7e25163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:34753/index.html?ui=P_0x7f13bc3862a0_20&reconnect=auto\" class=\"pyv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'blue'), (0.5, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "target_pv_cloud = pv.PolyData(filtered_results['data'][0])\n",
    "\n",
    "norm_xyz_sum = normalise_attributions(filtered_results['attributions_list'][0], power = 0.2)\n",
    "\n",
    "opacity_scalars = np.clip(np.abs(norm_xyz_sum - 0.5), 0.05 , 0.8)\n",
    "\n",
    "plotter.add_points(target_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim=[0,1], opacity=opacity_scalars)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_pv_cloud = pv.PolyData(np.asarray(moving_pcd.points))\n",
    "\n",
    "    norm_xyz_sum = normalise_attributions(filtered_results['attributions_list'][i], power = 0.2)\n",
    "\n",
    "    opacity_scalars = np.clip(np.abs(norm_xyz_sum - 0.5), 0.05 , 0.8)\n",
    "\n",
    "    plotter.add_points(transformed_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim= [0,1], opacity=opacity_scalars)\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global explainability for all correct predictions, with absolute attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointnet_eval = np.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/pointnet_eval.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == results['pred_classes'][i]:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_list': [results['attributions_list'][i] for i in correct_indices]\n",
    "}\n",
    "\n",
    "print(len(filtered_results['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1543634c4d104e7baa663225a12bb66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:34753/index.html?ui=P_0x7f139a005f70_21&reconnect=auto\" class=\"pyv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "target_pv_cloud = pv.PolyData(filtered_results['data'][0])\n",
    "\n",
    "# Sum x, y and z values for an overall attribution for that point\n",
    "xyz_sum = np.sum(filtered_results['attributions_list'][0], axis=1)\n",
    "\n",
    "# Apply power for better vis\n",
    "xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), 0.3)\n",
    "\n",
    "xyz_sum = np.abs(xyz_sum)\n",
    "\n",
    "norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "opacity_scalars = np.clip(norm_xyz_sum, 0.01 , 0.8)\n",
    "\n",
    "plotter.add_points(target_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim=[0,1], opacity=opacity_scalars)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_pv_cloud = pv.PolyData(np.asarray(moving_pcd.points))\n",
    "\n",
    "    # Sum x, y and z values for an overall attribution for that point\n",
    "    xyz_sum = np.sum(filtered_results['attributions_list'][i], axis=1)\n",
    "\n",
    "    # Apply power for better vis\n",
    "    xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), 0.3)\n",
    "\n",
    "    xyz_sum = np.abs(xyz_sum)\n",
    "\n",
    "    norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "    opacity_scalars = np.clip(norm_xyz_sum, 0.01 , 0.8)\n",
    "\n",
    "    plotter.add_points(transformed_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim= [0,1], opacity=opacity_scalars)\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
