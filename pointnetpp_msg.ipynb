{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import *\n",
    "\n",
    "# Benny pointnet\n",
    "from pointnet2_benny import pointnet2_cls_msg\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Custom modules\n",
    "from preprocessing_post_fastsurfer.subject import *\n",
    "from preprocessing_post_fastsurfer.vis import *\n",
    "from ozzy_torch_utils.split_dataset import *\n",
    "from ozzy_torch_utils.SubjectDataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/scratch-disk/full-datasets/hcampus-large-cohort\"\n",
    "\n",
    "selected_labels = ['CN', 'MCI']\n",
    "\n",
    "# Dictionary key representing the data of interest\n",
    "data_string = 'hcampus_pointcloud'\n",
    "\n",
    "# Dictionary key representing the disease labels\n",
    "labels_string = 'research_group'\n",
    "\n",
    "# Prevent class imbalance\n",
    "downsample_majority = True\n",
    "\n",
    "# NB this argument makes prevent_id_leakage redundant\n",
    "single_img_per_subject = False\n",
    "\n",
    "# Prevent the same subject id from occuring in train and test, in case of more than one image per id\n",
    "prevent_id_leakage = True\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SubjectDataset(data_path, selected_labels, downsample_majority=downsample_majority, single_img_per_subject=single_img_per_subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data checks\n",
    "\n",
    "Check the size of the dataset and the number of unique labels and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset size: {len(dataset)}\\n\")\n",
    "\n",
    "labels = [dataset[index]['research_group'] for index in range(len(dataset.subject_list))]\n",
    "\n",
    "ids = [dataset.subject_list[index].subject_metadata['Subject'] for index in range(len(dataset.subject_list))]\n",
    "\n",
    "print(f\"Unique labels: {np.unique(labels, return_counts=True)}\\n\")\n",
    "\n",
    "print(f\"Unique ids: {np.unique(ids, return_counts=True)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, test_data = split_dataset(dataset, test_size=test_size, prevent_id_leakage=prevent_id_leakage)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn([data_string, labels_string]))\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn([data_string, labels_string]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data checks\n",
    "Check if there are subjects split across train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ids = [dataset.subject_list[index].subject_metadata['Subject'].iloc[0] for index in train_data.indices]\n",
    "\n",
    "test_ids = [dataset.subject_list[index].subject_metadata['Subject'].iloc[0] for index in test_data.indices]\n",
    "\n",
    "print(f\"Id intersection between train and test: {np.intersect1d(np.unique(train_ids), np.unique(test_ids))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuda setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "w/c 25/02\n",
    "- Strange issue where first run (from restart) of the model gives convincing results\n",
    "- After this, all epochs (even on restart) predict entirely one class and this class swaps over\n",
    "- Downsampling majority class hasn't fixed this\n",
    "- Could this be to do with argmax. How should I convert outputs into classes\n",
    "- Could it be overfitting? Do i need more data\n",
    "- After 7 epochs, got 65% accuracy with non downsampled majority, learning rate of 0.0001\n",
    "\n",
    "w/c 3/03\n",
    "- Got more data\n",
    "- Adjusted sampling of both datasets to use uniform sampling at 2048 samples\n",
    "- Initial dataset with new sampling results looked similar (first few epochs)\n",
    "- Large dataset with new sampling results looked similar (first few epochs)\n",
    "- Validation loss is much larger than training loss, could mean overfitting\n",
    "- Could overfitting be to do with lack of smoothing after walking cubes?\n",
    "- Training with only one image per subject gives approx 200 subjects, seems to overfit and train poorly\n",
    "- Would similar images of the same subject cause overfitting or could it be considered data augmentation?\n",
    "\n",
    "6/05\n",
    "- Trained multiple models overnight, when preventing data leakage accuracy is poor and overfitting is happening\n",
    "- Learning rate doesn't have much effect as all are overfitted\n",
    "- Single image per subject is also overfitted \n",
    "- Going to try with ssg rather than msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"training_losses\" : [],\n",
    "    \"validation_losses\": [],\n",
    "    \"conf_matrices\": [],\n",
    "    \"accuracies\": [],\n",
    "    \"f1s\": [],\n",
    "    \"precisions\": [],\n",
    "    \"recalls\": [],\n",
    "    \"train_time\": None,\n",
    "    \"num_training_images\": None\n",
    "}\n",
    "\n",
    "model = pointnet2_cls_msg.get_model(dataset.num_classes, normal_channel=False)\n",
    "\n",
    "criterion = pointnet2_cls_msg.get_loss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=1e-4,\n",
    "            amsgrad=True\n",
    "        )\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f\"Starting epoch {epoch + 1}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, dict in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        \n",
    "        # Access dict returned by dataset __getitem__\n",
    "        points = dict[data_string]\n",
    "        labels = dict[labels_string]\n",
    "        \n",
    "        # Transpose as in benny script (NB why does it need a transpose)\n",
    "        points = points.transpose(2, 1)\n",
    "        \n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output, _ = model(points)\n",
    "\n",
    "        # Calculate loss, trans_feat argument as None as not used in this function\n",
    "        loss = criterion(output, labels, None)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Multiply loss by batch size to account for differences in batch size (e.g last batch)\n",
    "        running_loss += loss.item() * points.size(0)\n",
    "        \n",
    "    metrics['training_losses'].append(running_loss/len(train_dataloader))\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialise loop metrics\n",
    "    running_loss = 0.0; conf_matrix = BinaryConfusionMatrix(); accuracy = BinaryAccuracy(); f1 = BinaryF1Score(); precision = BinaryPrecision(); recall = BinaryRecall(); auroc = BinaryAUROC()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_idx, dict in enumerate(test_dataloader):\n",
    "            \n",
    "            points = dict[data_string]\n",
    "            labels = dict[labels_string]\n",
    "            \n",
    "            points = points.transpose(2, 1)\n",
    "            \n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            \n",
    "            output, _ = model(points)\n",
    "            \n",
    "            running_loss += criterion(output, labels, None).item() * points.size(0)\n",
    "            \n",
    "            # Apply exponent as the output of the model is log softmax\n",
    "            pred_probability = torch.exp(output)\n",
    "            \n",
    "            # Threshold is variable to give preference to FN or FP\n",
    "            pred_labels = (pred_probability[:, 1] >= threshold).int()\n",
    "            \n",
    "            # Old label conversion\n",
    "            # pred_labels = torch.argmax(pred_probability, dim=-1)\n",
    "\n",
    "            # Update metrics\n",
    "            [metric.update(pred_labels, labels) for metric in [conf_matrix, accuracy, f1, precision, recall]]\n",
    "\n",
    "    end_time = datetime.now()\n",
    "            \n",
    "    # Append metric lists\n",
    "    [metrics[key].append(metric.compute()) for key, metric in [(\"conf_matrices\", conf_matrix), (\"accuracies\", accuracy), (\"f1s\", f1), (\"precisions\", precision), (\"recalls\", recall)]]       \n",
    "         \n",
    "    metrics['validation_losses'].append(running_loss/len(test_dataloader))\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} complete\\n\")\n",
    "    print(\"------------------------\")\n",
    "    print(conf_matrix.compute())\n",
    "    print(f\"Training Loss:   {metrics['training_losses'][-1]:.4f}\")\n",
    "    print(f\"Validation Loss: {metrics['validation_losses'][-1]:.4f}\")\n",
    "    print(f\"Accuracy:        {metrics['accuracies'][-1]:.4f}\")\n",
    "    print(f\"F1 Score:        {metrics['f1s'][-1]:.4f}\")\n",
    "    print(f\"Precision:       {metrics['precisions'][-1]:.4f}\")\n",
    "    print(f\"Recall:          {metrics['recalls'][-1]:.4f}\")\n",
    "    print(\"------------------------\\n\\n\")\n",
    "        \n",
    "    # Break before nightly restart\n",
    "    current_time = datetime.now()\n",
    "    \n",
    "    if current_time.hour == 23 and current_time.minute >= 30:\n",
    "        \n",
    "        print(\"Break before nightly restart\")\n",
    "        \n",
    "        break\n",
    "    \n",
    "metrics['train_time'] = end_time - start_time\n",
    "metrics['num_training_images'] = len(test_data)\n",
    "\n",
    "torch.save(model.state_dict(), 'trained_model.pth')\n",
    "\n",
    "print(\"Training complete and model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB this function has to remain in the notebook for it to work properly\n",
    "# Plot training loss, validation loss, and accuracy on separate subplots, along with displaying hyperparameters\n",
    "def plot(metrics, model_name, param_list, save=True, ylim=None):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "    ax1.plot(metrics['training_losses'], label='Training Loss', color='blue')\n",
    "    ax1.plot(metrics['validation_losses'], label='Validation Loss', color='red')\n",
    "    ax1.set_title('Training and Validation Loss over Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    if ylim is not None:\n",
    "        \n",
    "        ax1.set_ylim(ylim)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        ax1.set_ylim(0, metrics['training_losses'][0] + 5)\n",
    "        \n",
    "    ax1.grid(True)\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    ax2.plot(metrics['accuracies'], label='Accuracy', color='green')\n",
    "    ax2.plot(metrics['f1s'], label='F1 Score', color='blue')\n",
    "    ax2.plot(metrics['precisions'], label='Precision', color='red')\n",
    "    ax2.plot(metrics['recalls'], label='Recall', color='orange')\n",
    "    ax2.set_title('Metrics over epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Value')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    minutes = metrics['train_time'].seconds // 60\n",
    "\n",
    "    seconds = metrics['train_time'].seconds % 60\n",
    "\n",
    "    train_time_str = f\"Training time: {minutes:02d}m {seconds:02d}s\"\n",
    "\n",
    "    info = []\n",
    "    info.append(train_time_str)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        info.append(f\"Number of training images: {metrics['num_training_images']:.0f}\")\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(\"Error with num_training_images\")\n",
    "        \n",
    "        \n",
    "    info.append(f\"Model name: {model_name}\")\n",
    "    info.append(f\"Best accuracy: {max(metrics['accuracies']):.2f}\")\n",
    "    info.append(f\"Best F1 Score: {max(metrics['f1s']):.2f}\")\n",
    "    info.append(f\"Best Precision: {max(metrics['precisions']):.2f}\")\n",
    "    info.append(f\"Best Recall: {max(metrics['recalls']):.2f}\")\n",
    "    info.append(f\"Epoch with smallest validation loss: {metrics['validation_losses'].index(min(metrics['validation_losses'])):.0f}\")\n",
    "    info.append(\"\\n\\n\")\n",
    "\n",
    "    # Nasty hack using globals() to get variable names automatically\n",
    "    for param in param_list:\n",
    "        \n",
    "        for name, value in globals().items():\n",
    "            \n",
    "            if value is param and name not in [info_line.split(\":\")[0] for info_line in info]:\n",
    "                \n",
    "                info.append(f\"{name}: {value}\")\n",
    "    \n",
    "    info_text = \"\\n\".join(info)\n",
    "    \n",
    "    fig.text(0.5, 0.02, info_text, ha='center', va='top', wrap=True, fontsize=10)\n",
    "\n",
    "    if save:\n",
    "        \n",
    "        # Save the fig and the lists of values\n",
    "        current_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        \n",
    "        name = f\"plot_{current_time}\"\n",
    "        \n",
    "        with open(f'/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/figs/{name}.pkl', 'wb') as file:\n",
    "            \n",
    "            pickle.dump(metrics, file)\n",
    "        \n",
    "        plt.savefig(f'/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/figs/{name}.png', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot(metrics, \"pointnet2_cls_msg\", [selected_labels, data_string, labels_string, downsample_majority, single_img_per_subject, prevent_id_leakage, batch_size, test_size, learning_rate, num_epochs, threshold])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
