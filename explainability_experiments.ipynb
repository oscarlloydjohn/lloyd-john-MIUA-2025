{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torcheval.metrics import *\n",
    "\n",
    "# Benny pointnet\n",
    "from pointnet2_benny import pointnet2_cls_msg\n",
    "\n",
    "import dill as pickle\n",
    "import shap\n",
    "from captum.attr import *\n",
    "import pyvista as pv\n",
    "from matplotlib.colors import Normalize\n",
    "from random import sample\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Custom modules\n",
    "from preprocessing_post_fastsurfer.subject import *\n",
    "from preprocessing_post_fastsurfer.vis import *\n",
    "from ozzy_torch_utils.split_dataset import *\n",
    "from ozzy_torch_utils.subject_dataset import *\n",
    "from ozzy_torch_utils.plot import *\n",
    "from ozzy_torch_utils.train_nn import *\n",
    "from ozzy_torch_utils.model_parameters import *\n",
    "from ozzy_torch_utils.init_dataloaders import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is deprecated however was used for experiments with pointnet and captum. It is replaced by code in final_models_explainability.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Black box wrapper for pointnet, allowing it to take a single input with no batch dimension\n",
    "class PointNetWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        super(PointNetWrapper, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Add batch dim and transpose to 3 x n for pointnet\n",
    "        x = x.unsqueeze(0).transpose(2, 1)\n",
    "\n",
    "        # Predict, taking only output and not l3_layer from pointnet\n",
    "        x = self.model(x)[0]\n",
    "\n",
    "        return x\n",
    "\n",
    "# Use pointnet model to run a prediction on a numpy input, and return a string of the research group along with the output as a numpy array\n",
    "def get_prediction(model, input, device):\n",
    "\n",
    "    input = torch.from_numpy(input).type(torch.float32).to(device)\n",
    "\n",
    "    model = PointNetWrapper(model)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    mapping = {\n",
    "        0: 'CN',\n",
    "        1: 'MCI',\n",
    "    }\n",
    "\n",
    "    # Remove batch dim\n",
    "    output = model(input).squeeze(0)\n",
    "\n",
    "    pred_class = int(torch.argmax(torch.nn.functional.softmax(output, dim=0)).cpu().numpy())\n",
    "\n",
    "    pred_research_group = mapping.get(pred_class, -1)\n",
    "\n",
    "    output = output.detach().cpu().numpy()\n",
    "\n",
    "    return pred_research_group, pred_class, output\n",
    "\n",
    "# THIS FUNCTION IS DEPRECATED AND IS KEPT AS DEMONSTRATION\n",
    "def pointnet_ig_deprecated(model, cloud, device):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    wrapped_model = PointNetWrapper(model)\n",
    "\n",
    "    wrapped_model.to(device)\n",
    "\n",
    "    ig = IntegratedGradients(wrapped_model)\n",
    "\n",
    "    # NN expects float32 tensor on device\n",
    "    input = torch.from_numpy(cloud).type(torch.float32).to(device)\n",
    "\n",
    "    # Baseline is zeros, however could also be some kind of noise cloud\n",
    "    baseline = torch.zeros_like(input)\n",
    "\n",
    "    # NB do we always want target to be 1 or the predicted class?\n",
    "    attributions = ig.attribute(input, baseline, target=1)\n",
    "\n",
    "    # Move to CPU for processing\n",
    "    attributions = attributions.cpu().numpy()\n",
    "    \n",
    "    return attributions\n",
    "\n",
    "## THIS METHOD IS BETTER, BECAUSE WE WANT TO PASS THE MATRIX IN WHERE POINTS ARE FEATURES OTHERWISE IG WILL BE CALCULATING THE IMPORTANCE OF X,Y,Z\n",
    "def pointnet_ig(model, cloud, device):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Wrap model as pointnet_cls outputs a tuple for some reason\n",
    "    wrapped_model = lambda x: model(x)[0]\n",
    "\n",
    "    ig = IntegratedGradients(wrapped_model)\n",
    "\n",
    "    input = torch.from_numpy(cloud)\n",
    "\n",
    "    # NN expects float32 on cuda\n",
    "    input = input.type(torch.float32).to(device)\n",
    "\n",
    "    # Unsqueeze to add empty batch dimension then transpose  to 3 x n as expected by pointnet\n",
    "    input = input.unsqueeze(0).transpose(2, 1)\n",
    "\n",
    "    # Baseline is zeros, however could also be some kind of noise cloud\n",
    "    baseline = torch.zeros_like(input)\n",
    "\n",
    "    attributions = ig.attribute(input, baseline, target=1, internal_batch_size=1)\n",
    "    \n",
    "    # Transpose back to n x 3 and remove batch dim\n",
    "    attributions = attributions.transpose(1, 2).squeeze(0)\n",
    "    \n",
    "    # Move to CPU for processing\n",
    "    attributions = attributions.cpu().numpy()\n",
    "    \n",
    "    return attributions\n",
    "\n",
    "\n",
    "# THIS FUNCTION IS DEPRECATED AND IS KEPT AS DEMONSTRATION\n",
    "def pointnet_saliency_deprecated(model, cloud, device):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    wrapped_model = PointNetWrapper(model)\n",
    "\n",
    "    wrapped_model.to(device)\n",
    "\n",
    "    saliency = Saliency(wrapped_model)\n",
    "\n",
    "    # NN expects float32 tensor on device\n",
    "    input = torch.from_numpy(cloud).type(torch.float32).to(device)\n",
    "\n",
    "    # NB do we always want target to be 1 or the predicted class?\n",
    "    attributions = saliency.attribute(input, target=1, abs=False)\n",
    "\n",
    "    # Move to CPU for processing\n",
    "    attributions = attributions.cpu().numpy()\n",
    "    \n",
    "    return attributions\n",
    "\n",
    "def pointnet_saliency(model, cloud, device):\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Wrap model as pointnet_cls outputs a tuple for some reason\n",
    "    wrapped_model = lambda x: model(x)[0]\n",
    "\n",
    "    saliency = Saliency(wrapped_model)\n",
    "\n",
    "    input = torch.from_numpy(cloud)\n",
    "\n",
    "    # NN expects float32 on cuda\n",
    "    input = input.type(torch.float32).to(device)\n",
    "\n",
    "    # Unsqueeze to add empty batch dimension then transpose  to 3 x n as expected by pointnet\n",
    "    input = input.unsqueeze(0).transpose(2, 1)\n",
    "\n",
    "    attributions = saliency.attribute(input, target=1, abs=False)\n",
    "    \n",
    "    # Transpose back to n x 3 and remove batch dim\n",
    "    attributions = attributions.transpose(1, 2).squeeze(0)\n",
    "    \n",
    "    # Move to CPU for processing\n",
    "    attributions = attributions.cpu().numpy()\n",
    "    \n",
    "    return attributions\n",
    "\n",
    "def vis_attributions(attributions: np.ndarray, subject: Subject, cloud: np.ndarray, pred_research_group: str, plot_attributions: bool = False, power: float = 0.25) -> None:\n",
    "    \n",
    "    # Sum x, y and z values for an overall attribution for that point\n",
    "    xyz_sum = np.sum(attributions, axis=1)\n",
    "\n",
    "    if plot_attributions:\n",
    "\n",
    "        plt.plot(xyz_sum)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), power)\n",
    "\n",
    "    if plot_attributions:\n",
    "        \n",
    "        plt.plot(xyz_sum)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Normalise such that 0 attribution maps to 0.5 and the relative sizes of positive and negative attributions is preserved\n",
    "    def norm(data):\n",
    "\n",
    "        min = np.min(data)\n",
    "        max = np.max(data)\n",
    "\n",
    "        max_abs_val = np.max((np.abs(min), np.abs(max)))\n",
    "\n",
    "        return np.array([0.5 + (value / (2 * max_abs_val)) for value in data])\n",
    "\n",
    "\n",
    "    norm_xyz_sum = norm(xyz_sum)\n",
    "\n",
    "    if plot_attributions:\n",
    "        \n",
    "        plt.plot(xyz_sum)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Have to use custom cmap to force the 0 attributions to be white\n",
    "    colours = [(0, 'blue'), (0.5, 'white'), (1, 'red')]\n",
    "    custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "    pv_cloud = pv.PolyData(cloud)\n",
    "\n",
    "    plotter = pv.Plotter()\n",
    "\n",
    "    plotter.add_points(pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim= [0,1])\n",
    "\n",
    "    plotter.set_background(\"black\")\n",
    "\n",
    "    # THIS IS NOT FOR USE, IT IS RUNNING PREDICTIONS ON TRAINING DATA!!\n",
    "    plotter.add_text(\"This is just a test running on training data!\", color='white')\n",
    "    plotter.add_text(f\"True class: {str(subject.subject_metadata['Research Group'].iloc[0])} \\n Predicted class: {pred_research_group} \", color='white', position='upper_right')\n",
    "\n",
    "    plotter.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Csv files: ['/uolstore/home/users/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/scratch-disk/full-datasets/hcampus-1.5T-cohort/idaSearch_3_19_2025.csv']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = \"/uolstore/home/users/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/scratch-disk/full-datasets/hcampus-1.5T-cohort\"\n",
    "\n",
    "subject_list = find_subjects_parallel(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the significance of left and right hippocampi on models run with both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = sample(subject_list, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "pickle_pathname = \"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/runs/run_18-03-2025_18-04-04/run_18-03-2025_18-04-04_params.pkl\"\n",
    "\n",
    "with open(pickle_pathname, 'rb') as file:\n",
    "    \n",
    "    model_parameters = pickle.load(file)\n",
    "    \n",
    "model = model_parameters.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e63692a7884aa2b1a23bcd5b622148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f279dbbf1a0_0&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_cloud = np.load(os.path.join(subject.path, \"Left-Hippocampus_Right-Hippocampus_aligned_cropped_mesh_downsampledcloud.npy\"))\n",
    "\n",
    "lr_attributions = pointnet_ig(model, lr_cloud, device)\n",
    "\n",
    "lr_pred_research_group, lr_pred_class, _ = get_prediction(model, lr_cloud, device)\n",
    "\n",
    "vis_attributions(lr_attributions, subject, lr_cloud, lr_pred_research_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "pickle_pathname = \"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/runs/run_19-03-2025_16-12-19/run_19-03-2025_16-12-19_params.pkl\"\n",
    "\n",
    "with open(pickle_pathname, 'rb') as file:\n",
    "    \n",
    "    model_parameters = pickle.load(file)\n",
    "    \n",
    "model = model_parameters.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640cbc2571724eeba33a91e532053b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f26a61c5d00_1&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l_cloud = np.load(os.path.join(subject.path, \"Left-Hippocampus_aligned_cropped_mesh_downsampledcloud.npy\"))\n",
    "\n",
    "l_pred_research_group, l_pred_class, _ = get_prediction(model, l_cloud, device)\n",
    "\n",
    "l_attributions = pointnet_ig(model, l_cloud, device)\n",
    "\n",
    "vis_attributions(l_attributions, subject, l_cloud, l_pred_research_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting experiment comparing attributions from two permutations of the same cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "pickle_pathname = \"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/runs/run_18-03-2025_15-35-05/run_18-03-2025_15-35-05_params.pkl\"\n",
    "\n",
    "with open(pickle_pathname, 'rb') as file:\n",
    "    \n",
    "    model_parameters = pickle.load(file)\n",
    "    \n",
    "model = model_parameters.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = np.load(os.path.join(subject.path, \"Left-Hippocampus_aligned_cropped_mesh_downsampledcloud.npy\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05976154424048312\n",
      "0.11921980889266279\n"
     ]
    }
   ],
   "source": [
    "attributions_orig = pointnet_ig(model, cloud, device)\n",
    "\n",
    "pred_research_group_orig, _, _ = get_prediction(model, cloud, device)\n",
    "\n",
    "shuffler = np.random.permutation(cloud.shape[0])\n",
    "\n",
    "unshuffler = np.argsort(shuffler)\n",
    "\n",
    "cloud_shuffled = np.array([cloud[i] for i in shuffler])\n",
    "\n",
    "attributions_shuffled = pointnet_ig(model, cloud_shuffled, device)\n",
    "\n",
    "pred_research_group_shuffled, _, _ = get_prediction(model, cloud, device)\n",
    "\n",
    "cloud_unshuffled = np.array([cloud_shuffled[i] for i in unshuffler])\n",
    "\n",
    "attributions_unshuffled = np.array([attributions_shuffled[i] for i in unshuffler])\n",
    "\n",
    "attributions_diff = attributions_orig - attributions_unshuffled\n",
    "\n",
    "# NB can't really visualise attributions as they will be normalsied and look large\n",
    "print(np.max(np.abs(attributions_diff)))\n",
    "\n",
    "print(np.max(np.abs(attributions_orig)))\n",
    "\n",
    "if pred_research_group_orig != pred_research_group_shuffled:\n",
    "    \n",
    "    print(\"Research groups are different after shuffle\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad00aeb1d8946deb3a2cc5c1bc57f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f26a623f050_2&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf2c38d80a94b18970e3bfbe6029310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f26a620be60_3&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4ab054cb4a461ab225b78a2b84cfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f26a4ed0860_4&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_attributions(attributions_orig, subject, cloud, pred_research_group_orig)\n",
    "\n",
    "# These two should look identical if the method was correct\n",
    "vis_attributions(attributions_shuffled, subject, cloud_shuffled, pred_research_group_shuffled)\n",
    "\n",
    "vis_attributions(attributions_unshuffled, subject, cloud_unshuffled, pred_research_group_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between transposing the point cloud before or after passing to integrated gradients and saliency. Treating the points as features rather than the dimensions yields better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "pickle_pathname = \"/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/runs/run_18-03-2025_18-04-04/run_18-03-2025_18-04-04_params.pkl\"\n",
    "\n",
    "with open(pickle_pathname, 'rb') as file:\n",
    "    \n",
    "    model_parameters = pickle.load(file)\n",
    "    \n",
    "model = model_parameters.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7afb4bd3fd47df9dfc070234c11453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f26a62290a0_5&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_research_group, _, _ = get_prediction(model, cloud, device)\n",
    "\n",
    "attributions = pointnet_ig(model, cloud, device)\n",
    "\n",
    "vis_attributions(attributions, subject, cloud, pred_research_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd80d3f65575450391b3781f2e7ee884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f26a4ed3710_6&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pred_research_group, _, _ = get_prediction(model, cloud, device)\n",
    "\n",
    "attributions = pointnet_ig_deprecated(model, cloud, device)\n",
    "\n",
    "vis_attributions(attributions, subject, cloud, pred_research_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/.venv/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21799f23945c4cf5ad04b58940420da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f26a4ed2360_7&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_research_group, _, _ = get_prediction(model, cloud, device)\n",
    "\n",
    "attributions = pointnet_saliency(model, cloud, device)\n",
    "\n",
    "vis_attributions(attributions, subject, cloud, pred_research_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uolstore/home/student_lnxhome01/sc22olj/Compsci/year3/individual-project-COMP3931/individual-project-sc22olj/.venv/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cdf6af146c41008f2f7ecff8862c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:37721/index.html?ui=P_0x7f26a4f73bc0_8&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_research_group, _, _ = get_prediction(model, cloud, device)\n",
    "\n",
    "attributions = pointnet_saliency_deprecated(model, cloud, device)\n",
    "\n",
    "vis_attributions(attributions, subject, cloud, pred_research_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
