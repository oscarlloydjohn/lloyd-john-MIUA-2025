{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "w/c 25/02\n",
    "- Strange issue where first run (from restart) of the model gives convincing results\n",
    "- After this, all epochs (even on restart) predict entirely one class and this class swaps over\n",
    "- Downsampling majority class hasn't fixed this\n",
    "- Could this be to do with argmax. How should I convert outputs into classes\n",
    "- Could it be overfitting? Do i need more data\n",
    "- After 7 epochs, got 65% accuracy with non downsampled majority, learning rate of 0.0001\n",
    "\n",
    "w/c 3/03\n",
    "- Got more data\n",
    "- Adjusted sampling of both datasets to use uniform sampling at 2048 samples\n",
    "- Initial dataset with new sampling results looked similar (first few epochs)\n",
    "- Large dataset with new sampling results looked similar (first few epochs)\n",
    "- Validation loss is much larger than training loss, could mean overfitting\n",
    "- Could overfitting be to do with lack of smoothing after walking cubes?\n",
    "- Training with only one image per subject gives approx 200 subjects, seems to overfit and train poorly\n",
    "- Would similar images of the same subject cause overfitting or could it be considered data augmentation?\n",
    "\n",
    "6/05\n",
    "- Trained multiple models overnight, when preventing data leakage accuracy is poor and overfitting is happening\n",
    "- Learning rate doesn't have much effect as all are overfitted\n",
    "- Single image per subject is also overfitted \n",
    "- Going to try with ssg rather than msg\n",
    "\n",
    "11/05\n",
    "- sklearn reporting only one class in labels, added drop_last to dataloaders to prevent this\n",
    "- Also added recursive call in init_dataloaders to redo split if one tensor has only one class (this doesn't work with large batches, instead concat batches for roc auc )\n",
    "- Need to get ROC scores accumulating correctly\n",
    "\n",
    "13/05\n",
    "- Both CNNs on brain and hcampus are drastically overfitting, regardless of parameters (probably need to do augmentation and dropout)\n",
    "- Removed downsample_majority as this was downsampling before data split giving unbalanced classes\n",
    "- Issue with disk speed when running many models\n",
    "- Make sure that input to crossentropy loss is correct shape\n",
    "\n",
    "15/05\n",
    "- Most models on new 1.5t dataset trained poorly\n",
    "- Model with best loss curve was on aligned pointclouds with learning rate 0.01, normal crossentropyloss. Could suggest gradient landscape has local minima - could add noise to data?\n",
    "- The same model with learning rate 0.001 trained worse although other metrics were approximately the same\n",
    "- Same model with 0.001 learning rate and NLLLoss overfitted\n",
    "\n",
    "- Need to try some models with larger learning rate\n",
    "\n",
    "17/05\n",
    "- Feature importance of left hippocampus in gbdt is much greater than any other brain region, could try training on only left hippocampus\n",
    "- Ran some pointnet models on left hippocampus only, seemed to perform well (as well as both, and metrics were more stable)\n",
    "\n",
    "18/05\n",
    "- Ran lhcampus model with some augmentation (no scaling), convergence was much better\n",
    "- Same model with larger batch size and smaller learning rate seemed to overfit\n",
    "\n",
    "19/05\n",
    "- Explainability seems to work although is the power factor misleading for vis?\n",
    "- Current augmentation does not actually augment but just peturbs existing data without duplicating\n",
    "- Could apply permutations to data?\n",
    "- Used histgradientboostingclassifier for scores as it allows nans\n",
    "\n",
    "20/05\n",
    "- Explainability for non hist boosted trees on volumes was much more plausible, could compare feature importance attribute and shap values to validate\n",
    "- Why use shap? It allows for both local and global explanations, also it is has low complexity for small tabular data\n",
    "- Does the scale of different scores affect the explainability or the model? Do i need to normalise\n",
    "- Shap explainablity on pointnet seems to return values of zero. could this be because of the background being a single example? Cannot do multiple background samples because of permutation invariance\n",
    "- Is model with shifts added better or worse?\n",
    "- Should attributions always be calculated relative to the positive class or to the predicted class?\n",
    "- Tried pointnet ssg and seems to perform better than msg, although may need early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
