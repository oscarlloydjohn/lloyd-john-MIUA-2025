{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "import open3d as o3d\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from final_models_explainability.get_predictions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability evaluation by comparing left and right hippocampal attributions\n",
    "### Evaluating left and right hippocampus model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.load(\"lr_pointnet_eval.npz\")\n",
    "\n",
    "# Eval\n",
    "report = classification_report(results['true'], results['pred_classes'], target_names=['CN', 'MCI'])\n",
    "\n",
    "print(report)\n",
    "\n",
    "roc_auc = roc_auc_score(results['true'], results['pred_classes'])\n",
    "\n",
    "print(f\"roc_auc: {roc_auc}\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(results['true'], results['pred_probs'])\n",
    "\n",
    "conf_matrix = confusion_matrix(results['true'], results['pred_classes'])\n",
    "\n",
    "conf_matrix_disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['CN','MCI'])\n",
    "\n",
    "conf_matrix_disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate explainability for correct positive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "results = np.load(\"lr_pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == 1 and results['pred_classes'][i] == 1:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_zero_list': [results['attributions_zero_list'][i] for i in correct_indices],\n",
    "                    'attributions_mean_list': [results['attributions_mean_list'][i] for i in correct_indices]\n",
    "                    \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "transformed_clouds_concat = []\n",
    "attributions_concat = []\n",
    "opacity_scalars_concat = []\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(10)\n",
    "\n",
    "#o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "transformed_clouds_concat.extend(filtered_results['data'][0])\n",
    "\n",
    "norm_xyz_sum =  normalise_attributions(filtered_results['attributions_zero_list'][0], power=0.5)\n",
    "\n",
    "opacity_scalars_concat.extend(np.clip(norm_xyz_sum, 0.01 , 0.8))\n",
    "\n",
    "attributions_concat.extend(norm_xyz_sum)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_clouds_concat.extend(np.asarray(moving_pcd.points))\n",
    "\n",
    "    norm_xyz_sum = normalise_attributions(filtered_results['attributions_zero_list'][i], power=0.5)\n",
    "\n",
    "    opacity_scalars_concat.extend(np.clip(norm_xyz_sum, 0.01 , 0.8))\n",
    "\n",
    "    attributions_concat.extend(norm_xyz_sum)\n",
    "\n",
    "transformed_clouds_concat = np.array(transformed_clouds_concat)\n",
    "attributions_concat = np.array(attributions_concat)\n",
    "opacity_scalars_concat = np.array(opacity_scalars_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state = 42)\n",
    "cluster_labels = kmeans.fit_predict(transformed_clouds_concat)\n",
    "\n",
    "cluster_avg_attr = np.zeros_like(attributions_concat)\n",
    "\n",
    "cluster_0_mask = (cluster_labels == 0)\n",
    "cluster_1_mask = (cluster_labels == 1)\n",
    "\n",
    "points_0 = transformed_clouds_concat[cluster_0_mask]\n",
    "attributions_0 = attributions_concat[cluster_0_mask]\n",
    "opacities_0 = opacity_scalars_concat[cluster_0_mask]\n",
    "points_1 = transformed_clouds_concat[cluster_1_mask]\n",
    "attributions_1 = attributions_concat[cluster_1_mask]\n",
    "opacities_1 = opacity_scalars_concat[cluster_1_mask]\n",
    "\n",
    "mean_attr_0 = np.mean(attributions_0)\n",
    "mean_attr_1 = np.mean(attributions_1)\n",
    "\n",
    "print(mean_attr_0, mean_attr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "cloud_0 = pv.PolyData(points_0)\n",
    "cloud_1 = pv.PolyData(points_1)\n",
    "\n",
    "plotter.add_points(cloud_0, scalars=attributions_0, cmap=custom_cmap, clim= [0,1], opacity=opacities_0, show_scalar_bar=False)\n",
    "\n",
    "plotter.add_points(cloud_1, scalars=attributions_1, cmap=custom_cmap, clim= [0,1], opacity=opacities_1, show_scalar_bar=False)\n",
    "\n",
    "midpoint_0 = points_0.mean(axis=0)\n",
    "\n",
    "midpoint_1 = points_1.mean(axis=0)\n",
    "\n",
    "# NB labels need to be manually assigned by comparing shapes as k means keeps switching labels\n",
    "plotter.add_point_labels([midpoint_0.tolist()], [f\"Left: {mean_attr_0:.4f}\"], text_color='black', font_size=30, shape_color='white')\n",
    "plotter.add_point_labels([midpoint_1.tolist()], [f\"Right: {mean_attr_1:.4f}\"], text_color='black', font_size=30, shape_color='white')\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate explainability for all correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"lr_pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == results['pred_classes'][i]:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_zero_list': [results['attributions_zero_list'][i] for i in correct_indices],\n",
    "                    'attributions_mean_list': [results['attributions_mean_list'][i] for i in correct_indices]\n",
    "                    \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "transformed_clouds_concat = []\n",
    "attributions_concat = []\n",
    "opacity_scalars_concat = []\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(10)\n",
    "\n",
    "#o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "transformed_clouds_concat.extend(filtered_results['data'][0])\n",
    "\n",
    "# Sum x, y and z values for an overall attribution for that point\n",
    "xyz_sum = np.sum(filtered_results['attributions_zero_list'][0], axis=1)\n",
    "\n",
    "# Apply power for better vis\n",
    "xyz_sum = np.power(np.abs(xyz_sum), 0.5)\n",
    "\n",
    "norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "opacity_scalars_concat.extend(np.clip(norm_xyz_sum, 0.01 , 0.8))\n",
    "\n",
    "attributions_concat.extend(norm_xyz_sum)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_clouds_concat.extend(np.asarray(moving_pcd.points))\n",
    "\n",
    "    # Sum x, y and z values for an overall attribution for that point\n",
    "    xyz_sum = np.sum(filtered_results['attributions_zero_list'][i], axis=1)\n",
    "\n",
    "    # Apply power for better vis\n",
    "    xyz_sum = np.power(np.abs(xyz_sum), 0.5)\n",
    "\n",
    "    norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "    opacity_scalars_concat.extend(np.clip(norm_xyz_sum, 0.01 , 0.8))\n",
    "\n",
    "    attributions_concat.extend(norm_xyz_sum)\n",
    "\n",
    "transformed_clouds_concat = np.array(transformed_clouds_concat)\n",
    "attributions_concat = np.array(attributions_concat)\n",
    "opacity_scalars_concat = np.array(opacity_scalars_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state = 42)\n",
    "cluster_labels = kmeans.fit_predict(transformed_clouds_concat)\n",
    "\n",
    "cluster_avg_attr = np.zeros_like(attributions_concat)\n",
    "\n",
    "cluster_0_mask = (cluster_labels == 0)\n",
    "cluster_1_mask = (cluster_labels == 1)\n",
    "\n",
    "points_0 = transformed_clouds_concat[cluster_0_mask]\n",
    "attributions_0 = attributions_concat[cluster_0_mask]\n",
    "opacities_0 = opacity_scalars_concat[cluster_0_mask]\n",
    "points_1 = transformed_clouds_concat[cluster_1_mask]\n",
    "attributions_1 = attributions_concat[cluster_1_mask]\n",
    "opacities_1 = opacity_scalars_concat[cluster_1_mask]\n",
    "\n",
    "mean_attr_0 = np.mean(attributions_0)\n",
    "mean_attr_1 = np.mean(attributions_1)\n",
    "\n",
    "print(mean_attr_0, mean_attr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "cloud_0 = pv.PolyData(points_0)\n",
    "cloud_1 = pv.PolyData(points_1)\n",
    "\n",
    "plotter.add_points(cloud_0, scalars=attributions_0, cmap=custom_cmap, clim= [0,1], opacity=opacities_0, show_scalar_bar=False)\n",
    "\n",
    "plotter.add_points(cloud_1, scalars=attributions_1, cmap=custom_cmap, clim= [0,1], opacity=opacities_1, show_scalar_bar=False)\n",
    "\n",
    "midpoint_0 = points_0.mean(axis=0)\n",
    "\n",
    "midpoint_1 = points_1.mean(axis=0)\n",
    "\n",
    "# NB labels need to be manually assigned by comparing shapes as k means keeps switching labels\n",
    "plotter.add_point_labels([midpoint_0.tolist()], [f\"Left: {mean_attr_0:.4f}\"], text_color='black', font_size=30, shape_color='white')\n",
    "plotter.add_point_labels([midpoint_1.tolist()], [f\"Right: {mean_attr_1:.4f}\"], text_color='black', font_size=30, shape_color='white')\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of pointnet explainability by global explainability\n",
    "### Global explainability for correct positive predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == 1 and results['pred_classes'][i] == 1:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_zero_list': [results['attributions_zero_list'][i] for i in correct_indices],\n",
    "                    'attributions_mean_list': [results['attributions_mean_list'][i] for i in correct_indices]\n",
    "}\n",
    "\n",
    "print(len(filtered_results['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "target_pv_cloud = pv.PolyData(filtered_results['data'][0])\n",
    "\n",
    "norm_xyz_sum = normalise_attributions(filtered_results['attributions_zero_list'][0], power = 0.5)\n",
    "\n",
    "opacity_scalars = np.clip(np.abs(norm_xyz_sum), 0.05 , 0.8)\n",
    "\n",
    "plotter.add_points(target_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim=[0,1], opacity=opacity_scalars)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_pv_cloud = pv.PolyData(np.asarray(moving_pcd.points))\n",
    "\n",
    "    norm_xyz_sum = normalise_attributions(filtered_results['attributions_zero_list'][i], power = 0.5)\n",
    "\n",
    "    opacity_scalars = np.clip(np.abs(norm_xyz_sum), 0.05 , 0.8)\n",
    "\n",
    "    plotter.add_points(transformed_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim= [0,1], opacity=opacity_scalars)\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global explainability for correct negative predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == 0 and results['pred_classes'][i] == 0:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_zero_list': [results['attributions_zero_list'][i] for i in correct_indices],\n",
    "                    'attributions_mean_list': [results['attributions_mean_list'][i] for i in correct_indices]\n",
    "}\n",
    "\n",
    "print(len(filtered_results['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "target_pv_cloud = pv.PolyData(filtered_results['data'][0])\n",
    "\n",
    "norm_xyz_sum = normalise_attributions(filtered_results['attributions_zero_list'][0], power = 0.5)\n",
    "\n",
    "opacity_scalars = np.clip(np.abs(norm_xyz_sum), 0.05 , 0.8)\n",
    "\n",
    "plotter.add_points(target_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim=[0,1], opacity=opacity_scalars)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_pv_cloud = pv.PolyData(np.asarray(moving_pcd.points))\n",
    "\n",
    "    norm_xyz_sum = normalise_attributions(filtered_results['attributions_zero_list'][i], power = 0.5)\n",
    "\n",
    "    opacity_scalars = np.clip(np.abs(norm_xyz_sum), 0.05 , 0.8)\n",
    "\n",
    "    plotter.add_points(transformed_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim= [0,1], opacity=opacity_scalars)\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global explainability for all correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == results['pred_classes'][i]:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_zero_list': [results['attributions_zero_list'][i] for i in correct_indices],\n",
    "                    'attributions_mean_list': [results['attributions_mean_list'][i] for i in correct_indices]\n",
    "}\n",
    "\n",
    "print(len(filtered_results['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "target_pv_cloud = pv.PolyData(filtered_results['data'][0])\n",
    "\n",
    "# Sum x, y and z values for an overall attribution for that point\n",
    "xyz_sum = np.sum(filtered_results['attributions_zero_list'][0], axis=1)\n",
    "\n",
    "# Apply power for better vis\n",
    "xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), 0.5)\n",
    "\n",
    "xyz_sum = np.abs(xyz_sum)\n",
    "\n",
    "norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "opacity_scalars = np.clip(norm_xyz_sum, 0.01 , 0.8)\n",
    "\n",
    "plotter.add_points(target_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim=[0,1], opacity=opacity_scalars, show_scalar_bar=False)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_pv_cloud = pv.PolyData(np.asarray(moving_pcd.points))\n",
    "\n",
    "    # Sum x, y and z values for an overall attribution for that point\n",
    "    xyz_sum = np.sum(filtered_results['attributions_zero_list'][i], axis=1)\n",
    "\n",
    "    # Apply power for better vis\n",
    "    xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), 0.5)\n",
    "\n",
    "    xyz_sum = np.abs(xyz_sum)\n",
    "\n",
    "    norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "    opacity_scalars = np.clip(norm_xyz_sum, 0.01 , 0.8)\n",
    "\n",
    "    plotter.add_points(transformed_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim= [0,1], opacity=opacity_scalars, show_scalar_bar=False)\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Global explainability for all correct predictions (mean IG baseline rather than zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results into only the correct positive predictions\n",
    "\n",
    "results = np.load(\"pointnet_eval.npz\")\n",
    "\n",
    "correct_indices = []\n",
    "\n",
    "for i in range(len(results['true'])):\n",
    "\n",
    "    if results['true'][i] == results['pred_classes'][i]:\n",
    "\n",
    "        correct_indices.append(i)\n",
    "\n",
    "filtered_results = {\n",
    "                    'true': [results['true'][i] for i in correct_indices],\n",
    "                    'pred_probs': [results['pred_probs'][i] for i in correct_indices],\n",
    "                    'data': [results['data'][i] for i in correct_indices], \n",
    "                    'attributions_zero_list': [results['attributions_zero_list'][i] for i in correct_indices],\n",
    "                    'attributions_mean_list': [results['attributions_mean_list'][i] for i in correct_indices]\n",
    "}\n",
    "\n",
    "print(len(filtered_results['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first hippocampus as a template, add it to the plotter\n",
    "\n",
    "plotter = pv.Plotter()\n",
    "\n",
    "colours = [(0, 'white'), (1, 'red')]\n",
    "    \n",
    "custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colours)\n",
    "\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "target_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][0])\n",
    "\n",
    "target_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "target_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([target_pcd], point_show_normal=True)\n",
    "\n",
    "target_pv_cloud = pv.PolyData(filtered_results['data'][0])\n",
    "\n",
    "# Sum x, y and z values for an overall attribution for that point\n",
    "xyz_sum = np.sum(filtered_results['attributions_mean_list'][0], axis=1)\n",
    "\n",
    "# Apply power for better vis\n",
    "xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), 0.5)\n",
    "\n",
    "xyz_sum = np.abs(xyz_sum)\n",
    "\n",
    "norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "opacity_scalars = np.clip(norm_xyz_sum, 0.01 , 0.8)\n",
    "\n",
    "plotter.add_points(target_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim=[0,1], opacity=opacity_scalars, show_scalar_bar=False)\n",
    "\n",
    "# Align all other point clouds with the first, and display. Note, does normalisation of attributions in range 0,1 affect the output?\n",
    "for i in range(len(filtered_results['true']))[1:]:\n",
    "    \n",
    "    trans_init = np.eye(4)\n",
    "\n",
    "    moving_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    moving_pcd.points = o3d.utility.Vector3dVector(filtered_results['data'][i])\n",
    "\n",
    "    moving_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=20, max_nn=20))\n",
    "\n",
    "    moving_pcd.orient_normals_consistent_tangent_plane(5)\n",
    "\n",
    "    reg = o3d.pipelines.registration.registration_icp(moving_pcd, target_pcd, 5, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "\n",
    "    moving_pcd.transform(reg.transformation)\n",
    "\n",
    "    transformed_pv_cloud = pv.PolyData(np.asarray(moving_pcd.points))\n",
    "\n",
    "    # Sum x, y and z values for an overall attribution for that point\n",
    "    xyz_sum = np.sum(filtered_results['attributions_mean_list'][i], axis=1)\n",
    "\n",
    "    # Apply power for better vis\n",
    "    xyz_sum = np.sign(xyz_sum) * np.power(np.abs(xyz_sum), 0.5)\n",
    "\n",
    "    xyz_sum = np.abs(xyz_sum)\n",
    "\n",
    "    norm_xyz_sum =  xyz_sum/np.max(xyz_sum)\n",
    "\n",
    "    opacity_scalars = np.clip(norm_xyz_sum, 0.01 , 0.8)\n",
    "\n",
    "    plotter.add_points(transformed_pv_cloud, scalars=norm_xyz_sum, cmap=custom_cmap, clim= [0,1], opacity=opacity_scalars, show_scalar_bar=False)\n",
    "\n",
    "plotter.set_background(\"black\")\n",
    "\n",
    "plotter.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
